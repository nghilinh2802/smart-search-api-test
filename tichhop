# -*- coding: utf-8 -*-
"""Tichhopsearchbar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lRJlx7_UlSN3qz_gwhG1dHYk9tqKycCi
"""

pip install fastapi uvicorn pandas sentence-transformers firebase-admin

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import re
from difflib import get_close_matches
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import firebase_admin
from firebase_admin import credentials, firestore
import os

from google.colab import files
uploaded = files.upload()

app = FastAPI()

# Initialize Firebase
cred = credentials.Certificate("/content/pawdicteddatabase-firebase-adminsdk-fbsvc-31fff878be.json")  # Replace with your Firebase service account key
firebase_admin.initialize_app(cred)
db = firestore.client()

model = SentenceTransformer('all-MiniLM-L6-v2')

# Load product data from Firestore and create embeddings
def load_data_from_firestore():
    products_ref = db.collection("products")
    docs = products_ref.stream()
    data = []
    for doc in docs:
        product = doc.to_dict()
        product['id'] = doc.id
        data.append(product)
    return pd.DataFrame(data)

df = load_data_from_firestore()

# Preprocess data
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["clean_product_name"] = df["product_name"].fillna("").apply(clean_text)
df["clean_description"] = df["description"].fillna("").apply(clean_text)
df["clean_details"] = df["details"].fillna("").apply(clean_text)
# Weight product_name higher by repeating it
df["full_text"] = (df["clean_product_name"] + " " + df["clean_product_name"] + " " +
                   df["clean_description"] + " " + df["clean_details"])

def clean_price(price):
    if pd.isna(price):
        return 0
    try:
        return float(str(price).replace(",", "").strip())
    except:
        return 0
df["price_clean"] = df["price"].apply(clean_price)

def extract_parent_name(name):
    if pd.isna(name):
        return ""
    if '-' in name:
        return name.split('-')[0].strip()
    elif '(' in name:
        return name.split('(')[0].strip()
    else:
        return name.strip()
df["parent_product_name"] = df["product_name"].apply(extract_parent_name)

# Generate embeddings
product_texts = df["full_text"]
product_embeddings = model.encode(product_texts.tolist(), show_progress_bar=True)

# Keyword list for autocomplete and spell correction
unique_keywords = set()
for name in df["product_name"].dropna().unique():
    unique_keywords.add(clean_text(name))  # Add full product names
    for word in name.lower().split():
        unique_keywords.add(word)
categories_and_brands = [
    'Accessories', 'Apparel & Costume', 'Bed', 'Blanket', 'Brushes & Combs', 'Carriers & Kennels',
    'Cat', 'Collar', 'Collar & Leash', 'Costume', 'Dental care', 'Deodorant tools', 'Dog', 'Dry food',
    'Feeders', 'Flea and Tick control', 'Food', 'Hammock', 'Leash', 'Nail care', 'Pillow', 'Set',
    'Shampoo & Conditioner', 'Small Animal', 'Supplements & Vitamins', 'Toys', 'Training', 'Treat',
    'Wet food', 'catnip', '3 Peaks', 'BAM!', 'Barkbutler', 'Basil', 'Chuckit!', 'Coachi', 'M-PETS',
    'Mitag', 'Noble', 'PAW', 'Papa Pawsome', 'Papaw Cartel', 'Pawgypets', 'Pawsome Couture',
    'Pedigree', 'Pets at Home', 'QPets', 'Squeeezys', 'TOPDOG', 'Trixie', 'dog food', 'cat food'
]
for kw in categories_and_brands:
    parts = kw.split()
    unique_keywords.add(clean_text(kw))  # Add full phrases
    for p in parts:
        ckw = clean_text(p)
        if ckw:
            unique_keywords.add(ckw)

# Common misspellings dictionary
misspellings = {
    "fodd": "food",
    "catnipp": "catnip",
    "collor": "collar",
    "leesh": "leash"
}

# Spell correction
def correct_spelling(word, keyword_list):
    word_lower = word.lower()
    # Check misspellings dictionary first
    if word_lower in misspellings:
        return misspellings[word_lower]
    if word_lower in keyword_list:
        return word_lower
    matches = get_close_matches(word_lower, keyword_list, n=1, cutoff=0.6)  # Lowered cutoff
    return matches[0] if matches else word_lower

def correct_full_query(query, keyword_list):
    words = query.lower().split()
    corrected_words = [correct_spelling(w, keyword_list) for w in words]
    return " ".join(corrected_words)

# Autocomplete with partial matches
def autocomplete_prefix(input_text, keyword_list, limit=5):
    input_text = input_text.lower()
    suggestions = []
    # Exact prefix matches
    suggestions.extend(kw for kw in keyword_list if kw.startswith(input_text))
    # Partial matches within words
    suggestions.extend(kw for kw in keyword_list if input_text in kw and not kw.startswith(input_text))
    return sorted(set(suggestions))[:limit]

def autocomplete_with_correction(input_text):
    input_text = input_text.lower()
    corrected = correct_spelling(input_text, list(unique_keywords))
    if corrected and corrected != input_text:
        fallback = autocomplete_prefix(corrected, unique_keywords)
        return {"type": "spell_corrected", "input": input_text, "suggestions": fallback, "correction": corrected}
    suggestions = autocomplete_prefix(input_text, unique_keywords)
    if suggestions:
        return {"type": "autocomplete", "input": input_text, "suggestions": suggestions, "correction": None}
    return {"type": "no_match", "input": input_text, "suggestions": [], "correction": None}

# Parse price range
def parse_price_range(query):
    price_min = None
    price_max = None
    m = re.search(r'price\s+under\s+([\d.,]+)', query, re.IGNORECASE)
    if m:
        price_max = float(re.sub(r"[.,]", "", m.group(1)))
        query = re.sub(r'price\s+under\s+[\d.,]+', '', query, flags=re.IGNORECASE).strip()
        return query, price_min, price_max
    m = re.search(r'price\s+over\s+([\d.,]+)', query, re.IGNORECASE)
    if m:
        price_min = float(re.sub(r"[.,]", "", m.group(1)))
        query = re.sub(r'price\s+over\s+[\d.,]+', '', query, flags=re.IGNORECASE).strip()
        return query, price_min, price_max
    m = re.search(r'price\s+from\s+([\d.,]+)\s+to\s+([\d.,]+)', query, re.IGNORECASE)
    if m:
        price_min = float(re.sub(r"[.,]", "", m.group(1)))
        price_max = float(re.sub(r"[.,]", "", m.group(2)))
        query = re.sub(r'price\s+from\s+[\d.,]+\s+to\s+[\d.,]+', '', query, flags=re.IGNORECASE).strip()
        return query, price_min, price_max
    return query.strip(), price_min, price_max

# Save to Firestore
for idx, embedding in enumerate(product_embeddings):
    db.collection("product_embeddings").document(df.iloc[idx]["id"]).set({
        "product_id": df.iloc[idx]["id"],
        "embedding": embedding.tolist()
    })
print("Embeddings saved to Firestore.")

# Search with SBERT
def search_sbert(query, top_n=10, price_min=None, price_max=None):
    clean_query = clean_text(query)
    query_embedding = model.encode([clean_query])
    scores = cosine_similarity(query_embedding, product_embeddings).flatten()
    df_search = df.copy()
    df_search["score"] = scores
    if price_min is not None:
        df_search = df_search[df_search["price_clean"] >= price_min]
    if price_max is not None:
        df_search = df_search[df_search["price_clean"] <= price_max]
    grouped = df_search.groupby("parent_product_name").agg({
        "score": "max",
        "price_clean": "min",
        "id": "first"
    }).reset_index()
    grouped = grouped.sort_values(by="score", ascending=False).head(top_n)
    grouped = grouped.rename(columns={"price_clean": "price"})
    return grouped[["id", "parent_product_name", "price", "score"]]



# prompt: có thể thử test vài cái được không

# Example usage for testing:
# Test the search function
test_query = "cat food under 200000"
corrected_query, price_min, price_max = parse_price_range(test_query)
print(f"Original query: {test_query}")
print(f"Corrected query (after price parsing): {corrected_query}")
print(f"Price min: {price_min}, Price max: {price_max}")
search_results = search_sbert(corrected_query, top_n=5, price_min=price_min, price_max=price_max)
print("\nSearch Results:")
print(search_results)
# Test the autocomplete function
test_autocomplete_input = "cat f"
autocomplete_suggestions = autocomplete_prefix(test_autocomplete_input, list(unique_keywords), limit=5)
print(f"\nAutocomplete suggestions for '{test_autocomplete_input}':")
print(autocomplete_suggestions)
# Test spell correction
test_spell_correction_input = "catnit"
corrected_spell = correct_full_query(test_spell_correction_input, list(unique_keywords))
print(f"\nSpell correction for '{test_spell_correction_input}':")
corrected_spell

# API endpoints
class SearchRequest(BaseModel):
    query: str
    top_n: int = 10

class AutocompleteRequest(BaseModel):
    input_text: str
    limit: int = 5

@app.post("/search")
async def search_products(request: SearchRequest):
    query, price_min, price_max = parse_price_range(request.query)
    corrected_query = correct_full_query(query, unique_keywords)
    results = search_sbert(corrected_query, request.top_n, price_min, price_max)
    return {
        "corrected_query": corrected_query if corrected_query != query else None,
        "results": results.to_dict(orient="records")
    }

@app.post("/autocomplete")
async def autocomplete(request: AutocompleteRequest):
    result = autocomplete_with_correction(request.input_text)
    return result

# Testing framework
def run_tests():
    print("=== Running Search Pipeline Tests ===\n")

    # Test cases
    test_cases = [
        {
            "query": "dog food",
            "description": "Basic search for dog food",
            "price_min": None,
            "price_max": None
        },
        {
            "query": "cat fodd under 100000",
            "description": "Search with misspelling and price filter",
            "price_min": None,
            "price_max": 100000
        },
        {
            "query": "catnip toy",
            "description": "Search for specific product type",
            "price_min": None,
            "price_max": None
        },
        {
            "query": "dog leesh price from 50000 to 150000",
            "description": "Search with misspelling and price range",
            "price_min": None,
            "price_max": 150000
        }
    ]

    # Autocomplete test cases
    autocomplete_tests = [
        {"input": "dog f", "description": "Autocomplete for partial dog food"},
        {"input": "cat", "description": "Autocomplete for cat-related terms"},
        {"input": "catnipp", "description": "Autocomplete with misspelling"}
    ]

    # Run search tests
    for test in test_cases:
        print(f"\nTest: {test['description']}")
        print(f"Original query: {test['query']}")
        query, price_min, price_max = parse_price_range(test["query"])
        print(f"Parsed query: {query}, Price min: {price_min}, Price max: {price_max}")
        corrected_query = correct_full_query(query, unique_keywords)
        print(f"Corrected query: {corrected_query}")
        results = search_sbert(corrected_query, top_n=5, price_min=price_min, price_max=price_max)
        print("Results:")
        print(results.to_string(index=False))

    # Run autocomplete tests
    print("\n=== Autocomplete Tests ===")
    for test in autocomplete_tests:
        print(f"\nTest: {test['description']}")
        print(f"Input: {test['input']}")
        result = autocomplete_with_correction(test["input"])
        print(f"Result: {result}")

if __name__ == "__main__":
    run_tests()

import uvicorn
    import asyncio





from sklearn.metrics.pairwise import cosine_similarity

def find_similar_items(product_id, top_n=5):
    if product_id not in df['id'].values:
        return "Product ID not found."

    # Get the embedding of the target product
    target_embedding = df[df['id'] == product_id]['embeddings'].iloc[0].reshape(1, -1)

    # Calculate cosine similarity between the target product and all other products
    similarities = cosine_similarity(target_embedding, np.stack(df['embeddings'].values)).flatten()

    # Get the indices of the top_n most similar products (excluding the target product itself)
    # Use argpartition for efficiency if top_n is much smaller than the total number of products
    # Exclude the first element which is the product itself with similarity 1.0
    top_indices = np.argpartition(similarities, -(top_n + 1))[-(top_n + 1):]
    top_indices = top_indices[np.argsort(similarities[top_indices])][::-1]
    top_indices = [i for i in top_indices if df['id'].iloc[i] != product_id][:top_n]

    # Return the similar products
    similar_products = df.iloc[top_indices][['id', 'product_name', 'price']]
    return similar_products.to_dict(orient='records')

class SimilarItemsRequest(BaseModel):
    product_id: str
    top_n: int = 5

@app.post("/similar_items")
async def get_similar_items(request: SimilarItemsRequest):
    results = find_similar_items(request.product_id, request.top_n)
    if isinstance(results, str):
        raise HTTPException(status_code=404, detail=results)
    return results

import uvicorn
import nest_asyncio

nest_asyncio.apply()

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

!pip install pyngrok

